{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc90822-09b3-4eae-8e87-5498765ac51d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f5850857-8e6b-4baa-9a0d-2719d1ae198b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PATH_INBOX_EXP = f\"{BASE_VOLUME_PATH}/Pendientes/Expuestos\"\n",
    "PATH_INBOX_CONT = f\"{BASE_VOLUME_PATH}/Pendientes/Contratantes\"\n",
    "\n",
    "PATH_PROCESSED_EXP = f\"{BASE_VOLUME_PATH}/Procesados/Expuestos\"\n",
    "PATH_PROCESSED_CONT = f\"{BASE_VOLUME_PATH}/Procesados/Contratantes\"\n",
    "\n",
    "TABLE_BRONZE_EXP = \"bronze_dev.sctr_emision.expuestos_bronze\"\n",
    "TABLE_BRONZE_CONT = \"bronze_dev.sctr_emision.contratantes_bronze\"\n",
    "\n",
    "COLS_IDX_EXP = [1,2,3,5,6,7,8,9,10,11,12,13,18,19]\n",
    "COLS_NAM_EXP = ['POLIZA','F_INI_VIGEN_POLIZA','F_FIN_VIGEN_POLIZA','CERTIFICADO','F_INI_COBERT','F_FIN_COBERT',\n",
    "                'P_NOMBRE','S_NOMBRE','AP_PATERNO','AP_MATERNO','TIPO_DOC','NUM_DOC','YEAR_MOV','MONTH_MOV']\n",
    "\n",
    "COLS_IDX_CONT = [1,2,3,6,8,9]\n",
    "COLS_NAM_CONT = ['TIPO_DOC','NUM_DOC_CONT','CONTRATANTE','POLIZA','YEAR_MOV','MONTH_MOV']\n",
    "\n",
    "open_log(\"Bronze\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "logger.info(f\"üü† Iniciando proceso Bronze en Databricks. Cl√∫ster: {spark.conf.get('spark.databricks.clusterUsageTags.clusterId')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f728ba-fea3-4b17-a982-a0540b4567da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_list_files(process_name: str, path_process: str) -> list:\n",
    "    logger.info(f\"   üöÄ Iniciando proceso {process_name}...\")\n",
    "\n",
    "    logger.info(f\"   üìÇ Leyendo archivos de carpeta {path_process}...\")\n",
    "    try:\n",
    "        lista_archivos = dbutils.fs.ls(path_process)\n",
    "        archivos_excel = [f.path for f in lista_archivos if f.name.endswith(\".xlsx\")]\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"   ‚ö†Ô∏è No se pudo listar carpeta. {e}\")\n",
    "        return None\n",
    "\n",
    "    if not archivos_excel:\n",
    "        logger.info(\"   ‚ÑπÔ∏è No hay archivos pendientes.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"   üìÑ Se encontraron {len(archivos_excel)} archivos.\")\n",
    "\n",
    "    return archivos_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "146585a6-c628-4336-a12e-6b1b87fe7536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_excel_to_df_spark(file_path: str,  cols_ids: list, col_names: list) -> DataFrame:\n",
    "    # logger.info(f\"   üìñ Analizando archivo : {file_path}\")\n",
    "    dfs_sheets = []\n",
    "    \n",
    "    try:\n",
    "        path_local = file_path.replace(\"dbfs:\", \"\").replace(\"file:\", \"\")\n",
    "        # local_path = file_path.replace(\"dbfs:\", \"/dbfs\")\n",
    "        inspector = fastexcel.read_excel(path_local)\n",
    "        sheet_names = inspector.sheet_names\n",
    "        \n",
    "        for sheet in sheet_names:\n",
    "            try:\n",
    "                df_sheet = (spark.read.format(\"com.crealytics.spark.excel\")\n",
    "                            .option(\"header\", \"true\") \n",
    "                            .option(\"inferSchema\", \"false\")\n",
    "                            .option(\"dataAddress\", f\"'{sheet}'!\")\n",
    "                            .option(\"treatEmptyValuesAsNulls\", \"true\") \n",
    "                            # .option(\"maxByteArraySize\", 2147483647)\n",
    "                            .load(file_path)\n",
    "                           )\n",
    "                \n",
    "                source_cols = df_sheet.columns\n",
    "\n",
    "                max_idx_needed = max(cols_ids)\n",
    "                if max_idx_needed >= len(source_cols):\n",
    "                    logger.warning(f\"    ‚ö†Ô∏è Hoja '{sheet}' ignorada: Faltan columnas. Se necesita √≠ndice {max_idx_needed}, pero hay {len(source_cols)} columnas.\")\n",
    "                    continue\n",
    "\n",
    "                selected_source_cols = [source_cols[i] for i in cols_ids]\n",
    "                \n",
    "                df_sheet = df_sheet.select(*[F.col(c) for c in selected_source_cols])\n",
    "\n",
    "                current_selected_cols = df_sheet.columns\n",
    "                \n",
    "                for i, new_col in enumerate(col_names):\n",
    "                    df_sheet = df_sheet.withColumnRenamed(current_selected_cols[i], new_col)\n",
    "\n",
    "                df_sheet = df_sheet.na.drop(how=\"all\")\n",
    "                \n",
    "                for c in col_names:\n",
    "                    df_sheet = df_sheet.withColumn(c, F.col(c).cast(\"string\"))\n",
    "                \n",
    "                dfs_sheets.append(df_sheet)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"    üõë Error leyendo hoja '{sheet}' con Spark: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not dfs_sheets:\n",
    "            return None\n",
    "\n",
    "        full_df = dfs_sheets[0]\n",
    "        for d in dfs_sheets[1:]:\n",
    "            full_df = full_df.unionAll(d)\n",
    "            \n",
    "        full_df = full_df.withColumn(\"NOMBRE_ARCHIVO\", F.lit(Path(file_path).name)) \\\n",
    "                         .withColumn(\"FECHA_CARGA\", F.lit(PERIODO))\n",
    "        \n",
    "        return full_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"   ‚ùå Error cr√≠tico en archivo {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def ingest_excel_to_df_fastexcel(file_path: str,  cols_ids: list, col_names: list) -> DataFrame:\n",
    "    # logger.info(f\"   üìñ Analizando archivo : {file_path}\")\n",
    "    excel_path = f\"./{Path(file_path).parent.name}/{Path(file_path).name}\"\n",
    "    try:\n",
    "        path_local = file_path.replace(\"dbfs:\", \"\").replace(\"file:\", \"\")\n",
    "        inspector = fastexcel.read_excel(path_local)\n",
    "        sheet_names = inspector.sheet_names\n",
    "        dfs_sheets = []\n",
    "        dtype_map = {idx: \"string\" for idx in cols_ids}\n",
    "\n",
    "        for sheet in sheet_names:\n",
    "            try:\n",
    "                sheet = inspector.load_sheet_by_name(\n",
    "                        sheet, \n",
    "                        use_columns=cols_ids, \n",
    "                        dtypes=dtype_map\n",
    "                    )\n",
    "                pdf = sheet.to_pandas()\n",
    "\n",
    "                source_cols = pdf.columns\n",
    "\n",
    "                if len(source_cols) != len(col_names):\n",
    "                    print(f\"    ‚ö†Ô∏è Hoja '{sheet}' ignorada: Cantidad de columnas no coincide.\")\n",
    "                    continue\n",
    "                \n",
    "                pdf.columns = col_names\n",
    "                \n",
    "                for col in pdf.columns:\n",
    "                    if pdf[col].dtype == \"object\":\n",
    "                        pdf[col] = pdf[col].str.strip()\n",
    "\n",
    "                dfs_sheets.append(pdf)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"    üõë Error leyendo hoja '{sheet}' con Spark: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not dfs_sheets:\n",
    "            return None\n",
    "        \n",
    "        full_pdf = pd.concat(dfs_sheets, ignore_index=True)\n",
    "        schema = StructType([StructField(c, StringType(), True) for c in col_names])\n",
    "        df_spk = spark.createDataFrame(full_pdf, schema=schema)\n",
    "        \n",
    "        full_df = (\n",
    "            df_spk\n",
    "            .withColumn(\"NOMBRE_ARCHIVO\",F.lit(excel_path)) \n",
    "            .withColumn(\"FECHA_CARGA\", F.lit(PERIODO))\n",
    "        )\n",
    "        \n",
    "        return full_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"   ‚ùå Error cr√≠tico en archivo {excel_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cbcc179-bfb0-4ff0-be32-6de2d2517ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_bronze_table_v2(process_name: str,  table_name: str, cols_ids: list, cols_nam: list, path_process_final: str, archivos_excel: list) -> bool:\n",
    "    status = False\n",
    "    dfs_spark = []\n",
    "    archivos_ok = []\n",
    "\n",
    "    try:\n",
    "        for file_uri in archivos_excel:\n",
    "            path_local = file_uri.replace(\"dbfs:\", \"\").replace(\"file:\", \"\")\n",
    "            \n",
    "            try:\n",
    "                # logger.info(f\"   üìñ Leyendo: {path_local.split('/')[-1]}\")\n",
    "                excel_reader = fastexcel.read_excel(path_local)\n",
    "                \n",
    "                pdf_list = []\n",
    "                dtype_map = {idx: \"string\" for idx in cols_ids}\n",
    "\n",
    "                for sheet_name in excel_reader.sheet_names:\n",
    "                    sheet = excel_reader.load_sheet_by_name(\n",
    "                        sheet_name, \n",
    "                        use_columns=cols_ids, \n",
    "                        dtypes=dtype_map\n",
    "                    )\n",
    "                    pdf = sheet.to_pandas()\n",
    "\n",
    "                    if len(pdf.columns) != len(cols_nam):\n",
    "                        print(f\"    ‚ö†Ô∏è Hoja '{sheet_name}' ignorada: Cantidad de columnas no coincide.\")\n",
    "                        continue\n",
    "                    \n",
    "                    pdf.columns = cols_nam\n",
    "                    \n",
    "                    for col in pdf.columns:\n",
    "                        if pdf[col].dtype == \"object\":\n",
    "                            pdf[col] = pdf[col].str.strip()\n",
    "\n",
    "                    pdf_list.append(pdf)\n",
    "\n",
    "                if not pdf_list:\n",
    "                    continue\n",
    "                \n",
    "                full_pdf = pd.concat(pdf_list, ignore_index=True)\n",
    "                schema = StructType([StructField(c, StringType(), True) for c in cols_nam])\n",
    "                df_spk = spark.createDataFrame(full_pdf, schema=schema)\n",
    "\n",
    "                df_spk = df_spk.withColumn(\"FECHA_CARGA\", F.lit(PERIODO))\n",
    "\n",
    "                dfs_spark.append(df_spk)\n",
    "                archivos_ok.append(file_uri)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"   ‚ùå Error en archivo: {path_local}. {e}\")\n",
    "                continue\n",
    "\n",
    "        if dfs_spark:\n",
    "            logger.info(\"   üîÑ Unificando datos...\")\n",
    "            final_df = dfs_spark[0]\n",
    "            for d in dfs_spark[1:]:\n",
    "                final_df = final_df.unionAll(d)\n",
    "                \n",
    "            logger.info(f\"   üíæ Guardando en Tabla Delta ({table_name})...\")\n",
    "            status = save_to_table_delta(final_df, table_name, \"append\", \"false\")\n",
    "\n",
    "            if not status:\n",
    "                return False\n",
    "            \n",
    "            logger.info(\"   üì¶ Moviendo archivos Procesados...\")\n",
    "            for src in archivos_ok:\n",
    "                file_name = src.split('/')[-1]\n",
    "                dst = f\"{path_process_final}/{file_name}\"\n",
    "                dbutils.fs.mv(src, dst)\n",
    "                \n",
    "        logger.info(f\"   ‚úÖ Proceso {process_name} terminado.\")\n",
    "        status = True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"   ‚ùå Error cr√≠tico ingestando Bronze ({process_name}). {e}\")\n",
    "        status = False\n",
    "\n",
    "    return status\n",
    "\n",
    "def save_bronze_table(process_name: str,  table_name: str, cols_ids: list, cols_nam: list, path_process_final: str, archivos_excel: list) -> bool:\n",
    "    status = False\n",
    "    files_processed = 0\n",
    "    total_rows = 0\n",
    "\n",
    "    try:\n",
    "        for file_uri in archivos_excel:\n",
    "            file_name = file_uri.split('/')[-1]\n",
    "\n",
    "            # gc.collect()\n",
    "\n",
    "            # df = ingest_excel_to_df_spark(file_uri, cols_ids, cols_nam)\n",
    "            df = ingest_excel_to_df_fastexcel(file_uri, cols_ids, cols_nam)\n",
    "\n",
    "            if not df is None:\n",
    "                # df.persist()\n",
    "        \n",
    "                status = save_to_table_delta(df, table_name, \"append\", \"false\")\n",
    "\n",
    "                if not status:\n",
    "                    del df\n",
    "                    gc.collect()\n",
    "                    return False\n",
    "                \n",
    "                logger.info(f\"   üì¶ Moviendo archivo Procesado {file_name}...\")\n",
    "                dst = f\"{path_process_final}/{file_name}\"\n",
    "                dbutils.fs.mv(file_uri, dst)\n",
    "                # dbutils.fs.cp(file_uri, dst)\n",
    "                files_processed += 1\n",
    "                total_rows += df.count()\n",
    "                del df\n",
    "            \n",
    "            gc.collect()\n",
    "\n",
    "        if files_processed == 0:\n",
    "            logger.warning(f\"   ‚ö†Ô∏è No se pudo cargar ning√∫n archivo en la tabla {table_name}.\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(f\"   üìä Total Registros Guardados: {total_rows:,.0f}\")\n",
    "\n",
    "        logger.info(f\"   ‚úÖ Proceso {process_name} terminado. Archivos cargados: {files_processed}/{len(archivos_excel)}\")\n",
    "        status = True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"   ‚ùå Error cr√≠tico ingestando Bronze ({process_name}). {e}\")\n",
    "        status = False\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d1ff045e-3351-4449-aab8-2449f7474056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def start_process(process: str, folder_path : str, table_name: str, cols_ids: list, cols_nam: list, path_process_final: str) -> bool:\n",
    "    status = False\n",
    "    excel_files = get_list_files(process, folder_path)\n",
    "    if excel_files:\n",
    "        status = save_bronze_table(process, table_name, cols_ids, cols_nam, path_process_final, excel_files)\n",
    "    if  validate_table_delta(table_name):\n",
    "        logger.info(f\"   üßπ Optimizando tabla Bronze {process}...\")\n",
    "        spark.sql(f\"OPTIMIZE {table_name}\")\n",
    "        status = True\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48139ac0-528d-410e-a45c-94c58f7506e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RUN_EXPUESTOS = True\n",
    "RUN_CONTRATANTES = True\n",
    "STATUS = False\n",
    "\n",
    "try:\n",
    "    if RUN_EXPUESTOS:\n",
    "        STATUS = start_process(\"Expuestos\", PATH_INBOX_EXP, TABLE_BRONZE_EXP, COLS_IDX_EXP, COLS_NAM_EXP, PATH_PROCESSED_EXP)\n",
    "\n",
    "    if RUN_CONTRATANTES:\n",
    "        STATUS = start_process(\"Contratantes\", PATH_INBOX_CONT, TABLE_BRONZE_CONT, COLS_IDX_CONT, COLS_NAM_CONT, PATH_PROCESSED_CONT)\n",
    "\n",
    "    if STATUS:\n",
    "        logger.success(\"üèÅ Ejecuci√≥n Completa: Proceso Bronze Finalizado con √©xito.\")\n",
    "    else:\n",
    "        logger.error(\"üèÅ Ejecuci√≥n Incompleta: Proceso Bronze Finalizado con Error.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Error cr√≠tico en proceso Bronze. {e}\")\n",
    "finally:\n",
    "    HORA_FINAL = datetime.now()\n",
    "    difference_time = HORA_FINAL-HORA_INICIAL\n",
    "    total_seconds = int(difference_time.total_seconds())\n",
    "    difference_formated = \"{} minuto(s), {} segundo(s)\".format((total_seconds // 60), total_seconds % 60)\n",
    "    logger.info(f\"Tiempo de proceso: {difference_formated}\")\n",
    "    close_log()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LoadBronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
